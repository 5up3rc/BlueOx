#!/usr/bin/python
import argparse
import bz2
import datetime
import io
import logging
import os
import os.path
import re
import socket
import sys

try:
    import boto
except ImportError:
    boto = None
else:
    from boto.s3.connection import OrdinaryCallingFormat

from blueox import store


DEFAULT_LOG_PATH = "/var/log/blueox"
DEFAULT_RETAIN_DAYS = 7

ZIP_COMMAND = "zip"
PRUNE_COMMAND = "prune"
ARCHIVE_COMMAND = "archive"
UPLOAD_COMMAND = "upload"
DOWNLOAD_COMMAND = "download"
CAT_COMMAND = "cat"

log = logging.getLogger(__name__)

class _LogFile(object):
    """Represents a remote log file"""

    def __init__(self, type_name, date):
        self.type_name = type_name
        self.date = date

    def get_s3_prefix(self):
        yyyymmdd = self.date.strftime('%Y%m%d')
        return '{yyyymmdd}/{type_name}-{yyyymmdd}'.format(
            type_name=self.type_name,
            yyyymmdd=yyyymmdd)


    def get_s3_storage_path(self, bzip2=False, hostname=None):
        """Get the path we store a log file for this machine on S3
        """
        base_name = '{}-{}.log'.format(
            self.get_s3_prefix(),
            socket.gethostname())
        if bzip2:
            return base_name + '.bz2'
        else:
            return base_name

    def s3_keys(self, bucket):
        """Get all of the S3 keys matching this log file"""
        log.debug("prefix: %s", self.get_s3_prefix())
        return bucket.get_all_keys(prefix=self.get_s3_prefix())

    def local_path(self, log_path, bzip2=False):
        return os.path.join(
            log_path,
            self.date.strftime('%Y%m%d'),
            self.filename(bzip2=bzip2))

    @classmethod
    def from_filename(cls, filename):
        basename = os.path.basename(filename)
        match = re.match(
            r"^(?P<stream>.+)" # stream name like "nginx-error"
            r"\-(?P<date>\d{8})" # date like 20140229
            r"(?:\-.+)?" # optional server name
            r"\.log" # .log
            r"(?:\.bz2|\.gz)?$",
            basename)
        if match is None:
            raise ValueError(basename)
        match_info = match.groupdict()
        a_date = datetime.datetime.strptime(match_info["date"], '%Y%m%d')
        return LogFile(match_info["stream"], a_date)


def inclusive_date_range(start_date, end_date):
    if end_date is None:
        end_date = start_date
    while start_date <= end_date:
        yield start_date
        start_date += datetime.timedelta(days=1)


def generate_log_files(type_name, start_date, end_date=None):
    for d in inclusive_date_range(start_date, end_date):
        log = LogFile(type_name, d)
        yield log


def generate_log_file_s3_keys(bucket, type_name, start_date, end_date=None):
    for log_file in generate_log_files(
            type_name,
            start_date,
            end_date=end_date):
        for key in log_file.s3_keys(bucket):
            log.debug(key.name)
            yield key


def setup_logging(verbose_count):
    level = logging.WARNING
    if verbose_count > 0:
        level = logging.INFO
    if verbose_count > 1:
        level = logging.DEBUG

    logging.basicConfig(level=level, format="%(asctime)-15s %(message)s")


def do_upload(log_path, bucket):
    """Upload available local log files to S3"""
    log_files = store.list_log_files(log_path)

    for lf in log_files:
        log.debug("Examining %s for archive", lf.file_path)
        if not lf.bzip:
            continue

        remote_lf = lf.build_remote(socket.gethostname())

        key = remote_lf.s3_key(bucket)

        log.debug("Checking for key %s", key)
        # TODO: Be nice to check for size differences
        # Might also be a good optimization to somehow download existing keys
        # in bulk, rather than make a request for each one.
        if not key.exists():
            with open(lf.get_local_file_path(log_path), "r") as fp:
                log.info("Uploading %s", key)
                key.set_contents_from_file(fp)
                log.debug("Done Uploading %s", key)
        else:
            log.debug("key %s already exists", key)


def do_download(bucket, type_name, start_dt, end_dt):
    "Run the download action"
    for log_file in store.find_log_files_in_s3(bucket, type_name, start_dt, end_dt):
        if os.path.exists(log_file.file_name):
            log.info("Skipping %s, exists", log_file.file_name)
        else:
            log.info("Downloading %s", log_file.file_name)
            log_file.s3_key(bucket).get_contents_to_filename(log_file.file_name)


def do_cat(bucket, log_path, type_name, start_date, end_date=None):
    "Run the cat action"
    for a_key in generate_log_file_s3_keys(
            bucket,
            type_name,
            start_date,
            end_date=end_date):
        if not a_key.exists():
            continue
        elif a_key.name.endswith(".bz2"):
            decompressor = bz2.BZ2Decompressor()
            for data in a_key:
                sys.stdout.write(decompressor.decompress(data))
        else:
            a_key.get_file(fp=sys.stdout)


def do_zip(log_path):
    log.debug("Listing log files for %r", args.log_path)
    log_files = store.list_log_files(args.log_path)

    for log_file in store.filter_log_files_for_zipping(log_files):
        log.info("Zipping %s", log_file.file_path)

        store.zip_log_file(log_file, args.log_path)


def do_prune(log_path, retain_days):
    local_archive_dirpath = ((
        datetime.datetime.utcnow() - datetime.timedelta(retain_days))
        .strftime('%Y%m%d'))

    for dirpath, dirnames, filenames in os.walk(log_path):
        if os.path.basename(dirpath) < local_archive_dirpath:
            for filename in filenames:
                full_path = os.path.join(dirpath, filename)
                log.info("Removing %s", full_path)
                os.unlink(full_path)

            os.rmdir(dirpath)


def parse_date_range_arguments(parser, args):
    if args.start is None:
        start_date = datetime.datetime.utcnow().replace(
            hour=0, minute=0, second=0, microsecond=0)
    else:
        try:
            start_date = store.parse_date_range_argument(args.start)
        except store.InvalidDateError:
            parser.error("Bad format for start date. Try YYYYMMDD")

    if args.end is None:
        end_date = start_date + datetime.timedelta(hours=24)
    else:
        try:
            end_date = store.parse_date_range_argument(args.end)
        except store.InvalidDateError:
            parser.error("Bad format for end date. Try YYYYMMDD")

    return start_date, end_date


def open_bucket(region_name, bucket_name):
    if boto is None:
        parser.error("boto library not available")

    # TODO: What kind of error handling / documentation around AWS creds?
    if False:
        required_keys = (
            'AWS_ACCESS_KEY_ID',
            'AWS_SECRET_ACCESS_KEY',
            'AWS_DEFAULT_REGION')

        for key in required_keys:
            if key not in os.environ:
                parser.error("Missing env var {}".format(key))


    # We need to specify calling_format to get around bugs in having a '.' in
    # the bucket name
    conn = boto.s3.connect_to_region(region_name, calling_format=OrdinaryCallingFormat())

    bucket = conn.get_bucket(bucket_name)
    if not bucket:
        parser.error("Missing bucket {}".format(bucket_name))

    return bucket


def main():
    parser = argparse.ArgumentParser(description='Manage blueox logs')
    parser.add_argument('--verbose', '-v', action='count')

    subparsers = parser.add_subparsers(
        dest='command', help='available commands')

    parser_zip = subparsers.add_parser(
        ZIP_COMMAND, help='Zip log files')
    parser_zip.add_argument(
        '--log-path', '-p', action='store', default=DEFAULT_LOG_PATH)

    parser_prune = subparsers.add_parser(
        PRUNE_COMMAND, help='Prune log files')
    parser_prune.add_argument(
        '--log-path', '-p', action='store', default=DEFAULT_LOG_PATH)
    parser_prune.add_argument(
        '--retain-days', '-d', action='store', type=int, default=DEFAULT_RETAIN_DAYS)

    parser_upload = subparsers.add_parser(
        UPLOAD_COMMAND, help='Upload local logs to S3')
    parser_upload.add_argument(
        '--bucket', '-b', action='store', required=True)
    parser_upload.add_argument(
        '--log-path', '-p', action='store', default=DEFAULT_LOG_PATH)

    parser_download = subparsers.add_parser(
        DOWNLOAD_COMMAND, help='Download logs from S3')
    parser_download.add_argument(
        '--bucket', '-b', action='store', required=True)
    parser_download.add_argument(
        '--start', '-s',
        action='store', help="start date (YYYYMMDD)")
    parser_download.add_argument(
        '--end', '-e', action='store', required=False, help="end date")
    parser_download.add_argument(
        'log_type', action='store', nargs=1, help="log type to download")

    if False:
        parser_cat = subparsers.add_parser(
            'cat', help='output specified log to stdout')

        parser_cat.add_argument(
            'log_type', action='store', nargs=1, help="log type")
        parser_cat.add_argument(
            '--start', '-s',
            action='store', required=True, help="start date (YYYYMMDD)")
        parser_cat.add_argument(
            '--end', '-e', action='store', required=False, help="end date")

    args = parser.parse_args()

    setup_logging(args.verbose)

    if args.command == ZIP_COMMAND:
        do_zip(args.log_path)
    elif args.command == PRUNE_COMMAND:
        do_prune(args.log_path, args.retain_days)
    elif args.command == UPLOAD_COMMAND:
        bucket = open_bucket('us-west-1', args.bucket)

        do_upload(args.log_path, bucket)
    elif args.command == DOWNLOAD_COMMAND:
        bucket = open_bucket('us-west-1', args.bucket)
        start_dt, end_dt = parse_date_range_arguments(parser, args)

        do_download(
            bucket,
            args.log_type[0],
            start_dt,
            end_dt)
    elif args.command == CAT_COMMAND:
        start_date, end_date = parse_date_range_arguments(parser, args)
        do_cat(
            bucket,
            args.log_path,
            args.log_type[0],
            start_date,
            end_date=end_date)
    else:
        parser.error("Unknown command")

    log.info("done")


if __name__ == '__main__':
    main()
